{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "lemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word_synset = wn.synsets(\"car\")\n",
    "print(\"synsets:\", word_synset)\n",
    "print(\"lemma names:\", word_synset[0].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "['he needs a car to get to work']\n",
      "a wheeled vehicle adapted to the rails of railroad\n",
      "['three cars had jumped the rails']\n"
     ]
    }
   ],
   "source": [
    "print(word_synset[0].definition())\n",
    "print(word_synset[0].examples())\n",
    "print(word_synset[1].definition())\n",
    "print(word_synset[1].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n"
     ]
    }
   ],
   "source": [
    "tree = wn.synsets(\"tree\")[0]\n",
    "paths = tree.hypernym_paths()\n",
    "for p in paths:\n",
    "  print([synset.name() for synset in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP.Lab3.helper.lemmatization.lemmatization_adapter import NLTKLemmatizationAdatper, LemmatizationAdatper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt lemmatizer to lemmatization Interface\n",
    "lemmatizer = NLTKLemmatizationAdatper(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@groovinshawn',\n",
       " 'they',\n",
       " 'are',\n",
       " 'rechargeable',\n",
       " 'and',\n",
       " 'it',\n",
       " 'normally',\n",
       " 'come',\n",
       " 'with',\n",
       " 'a',\n",
       " 'charger',\n",
       " 'when',\n",
       " 'u',\n",
       " 'buy',\n",
       " 'it',\n",
       " ':)']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from NLP.Lab3.logger import logging\n",
    "\n",
    "def lemmatize_sentence(tokens: Iterable, \n",
    "                       pos: str = 'v',\n",
    "                       lemmatizer: LemmatizationAdatper = lemmatizer,\n",
    "                       max_cache_size: int = 10000):\n",
    "  lemmatized_words = {}\n",
    "  \n",
    "  try:\n",
    "    for token in tokens:\n",
    "      if token in lemmatized_words:\n",
    "        yield token\n",
    "        \n",
    "      else:\n",
    "        if len(lemmatized_words) < max_cache_size:\n",
    "          lemmatized_words[token] = lemmatizer.lemmatize(token)\n",
    "          yield lemmatized_words[token]\n",
    "        else:\n",
    "          yield lemmatizer.lemmatize(token)\n",
    "      \n",
    "  except Exception as e:\n",
    "    logging.exception(e)\n",
    "\n",
    "list(lemmatize_sentence(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def process_tokens(tweet_tokens: Iterable,\n",
    "                   stop_words: set = set(stop_words),\n",
    "                   invalid_symbols: set = set(string.punctuation),\n",
    "                   invalid_pattern: str = r'(?:https?://\\S+|www\\.\\S+)|@\\w+',\n",
    "                   lemmatizer: LemmatizationAdatper = lemmatizer,\n",
    "                   replace_pattern: str = '#',\n",
    "                   replace_value: str = '',\n",
    "                   max_cache_size: int = 10000\n",
    "                   ):\n",
    "    \n",
    "    tokens_memo = {}\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = token.lower()\n",
    "        \n",
    "        if not (token in stop_words or token in invalid_symbols or re.search(invalid_pattern, token)):\n",
    "            if replace_pattern:\n",
    "                token = re.sub(replace_pattern, replace_value, token)\n",
    "                \n",
    "            if token in tokens_memo:\n",
    "                yield tokens_memo[token]\n",
    "            else:\n",
    "                if len(tokens_memo) < max_cache_size:\n",
    "                    tokens_memo[token] = lemmatizer.lemmatize(token)\n",
    "                    yield tokens_memo[token]\n",
    "                    \n",
    "                else:\n",
    "                    yield lemmatizer.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from NLP.Lab3.helper.synsets.synsets_adapter import NLTKSynsetsAdapter, SynsetsAdapter\n",
    "\n",
    "def process_tokens_(tweet_tokens: Iterable,\n",
    "                   stop_words: set = set(stop_words),\n",
    "                   invalid_symbols: set = set(string.punctuation),\n",
    "                   invalid_pattern: str = r'(?:https?://\\S+|www\\.\\S+)|@\\w+',\n",
    "                   synsets_processor: SynsetsAdapter = NLTKSynsetsAdapter(wn),\n",
    "                   replace_pattern: str = '#',\n",
    "                   replace_value: str = '',\n",
    "                   max_cache_size: int = 10000\n",
    "                   ):\n",
    "    \n",
    "    tokens_memo = {}\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = token.lower()\n",
    "        \n",
    "        if not (token in stop_words or token in invalid_symbols or re.search(invalid_pattern, token)):\n",
    "            if replace_pattern:\n",
    "                token = re.sub(replace_pattern, replace_value, token)\n",
    "                \n",
    "            if token in tokens_memo:\n",
    "                yield tokens_memo[token]\n",
    "            else:\n",
    "                if len(tokens_memo) < max_cache_size:\n",
    "                    processed_token = synsets_processor.synsets(token)\n",
    "                    tokens_memo[token] = processed_token[0].lemmas()[0].name() if processed_token else processed_token\n",
    "                    yield tokens_memo[token]\n",
    "                    \n",
    "                else:\n",
    "                    processed_token = synsets_processor.synsets(token)\n",
    "                    yield processed_token[0].lemmas()[0].name() if processed_token else processed_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After\", list(process_tokens(tweet_tokens[50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: [[], [], 'come', [], [], 'buy', []]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", list(process_tokens_(tweet_tokens[50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic distance between 'dog' and 'cat': 4\n"
     ]
    }
   ],
   "source": [
    "# Function to compute semantic distance between two words\n",
    "def semantic_distance(word1, word2):\n",
    "    # Get the synsets for both words\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return \"No synsets found for one or both words.\"\n",
    "\n",
    "    # Initialize minimum distance as infinity\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    # Iterate over all synset combinations (cross-product of both words' synsets)\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            # Get the hypernyms of both synsets\n",
    "            hypernyms1 = synset1.hypernyms()\n",
    "            hypernyms2 = synset2.hypernyms()\n",
    "\n",
    "            # Compute path similarity between the two hypernyms\n",
    "            for hyper1 in hypernyms1:\n",
    "                for hyper2 in hypernyms2:\n",
    "                    # Find the shortest path length to the common ancestor (root)\n",
    "                    distance = synset1.shortest_path_distance(synset2)\n",
    "                    if distance and distance < min_distance:\n",
    "                        min_distance = distance\n",
    "    \n",
    "    if min_distance == float('inf'):\n",
    "        return \"No semantic path found.\"\n",
    "    \n",
    "    return min_distance\n",
    "\n",
    "# Example usage\n",
    "word1 = \"dog\"\n",
    "word2 = \"cat\"\n",
    "print(f\"Semantic distance between '{word1}' and '{word2}': {semantic_distance(word1, word2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'be', 'run', 'fast', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nazarlenisin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert the POS tag from treebank format to WordNet format.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_tokens(text):\n",
    "    \"\"\"\n",
    "    Process the tokens in the text by finding the most appropriate WordNet synset.\n",
    "    \"\"\"\n",
    "    # Tokenize the text and get POS tags\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    processed_tokens = []\n",
    "    \n",
    "    for word, tag in tagged_tokens:\n",
    "        # Get the corresponding WordNet POS tag\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        \n",
    "        if wordnet_pos:\n",
    "            # Find synsets for the word with the specific part of speech\n",
    "            synsets = wn.synsets(word, pos=wordnet_pos)\n",
    "            \n",
    "            if synsets:\n",
    "                # Pick the first synset (you can use more sophisticated selection strategies here)\n",
    "                best_synset = synsets[0]\n",
    "                # Get the lemma name (the most canonical form of the word)\n",
    "                processed_tokens.append(best_synset.lemmas()[0].name())\n",
    "            else:\n",
    "                # If no synsets found, append the original word\n",
    "                processed_tokens.append(word)\n",
    "        else:\n",
    "            # If no suitable POS found, just append the original word\n",
    "            processed_tokens.append(word)\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Example usage:\n",
    "text = \"The dogs are running fast.\"\n",
    "processed = process_tokens(text)\n",
    "print(processed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
