{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 0 | Task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Introducing Interface of text preprocessor\n",
    "class TextPreprocessorI(Protocol):\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> list[str]:\n",
    "        ...\n",
    "        \n",
    "        \n",
    "# Inplementation of concrete text preprocessor\n",
    "class TextPreprocessor:\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> list[str]:\n",
    "        return word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Machine learning is revolutionizing industries by enabling computers to learn from data and make intelligent decisions',\n",
    "          'Ever wondered how Netflix knows what to recommend? That’s supervised learning! But how does an algorithm find patterns in customer behavior without labels? That’s unsupervised learning. Explore the differences and their real-world applications!',\n",
    "          ' Neural networks mimic the human brain, enabling AI to recognize speech, translate languages, and even generate realistic images. Deep learning is unlocking new possibilities in healthcare, finance, and entertainment. Are you keeping up with the AI revolution?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and make intelligent decisions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NGramsModel:\n",
    "    def __init__(self, n: int = 2, text_preprocessor: TextPreprocessorI = TextPreprocessor):\n",
    "        self.n = n\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self._backoff_memo = {}\n",
    "        self._eos_token = '<eos>'\n",
    "        \n",
    "        \n",
    "    def _build_vocab(self, corpus: list[str]) -> None:\n",
    "        self.vocab = set()\n",
    "        self._tokenized_text = []\n",
    "        \n",
    "        for text in corpus:\n",
    "            processed_text = self.text_preprocessor.preprocess(text)\n",
    "            self.vocab.update(processed_text)\n",
    "            self._tokenized_text.append(processed_text)\n",
    "        \n",
    "        self.vocab.add(self._eos_token)\n",
    "        \n",
    "        \n",
    "    def _backoff(self, provided_context: list[str]) -> int:\n",
    "        if tuple(provided_context) in self._backoff_memo:\n",
    "            return self._backoff_memo[tuple(provided_context)]\n",
    "        \n",
    "        provided_context_copy = provided_context.copy()\n",
    "       \n",
    "        \n",
    "        while provided_context_copy:\n",
    "            provided_context_copy.pop()\n",
    "            if tuple(provided_context_copy) in self.context:\n",
    "                self._backoff_memo[tuple(provided_context)] = self.context[tuple(provided_context_copy)]\n",
    "                return self._backoff_memo[tuple(provided_context)]\n",
    "            \n",
    "        self._backoff_memo[tuple(provided_context)] = len(self.vocab)\n",
    "        return self._backoff_memo[tuple(provided_context)]\n",
    "         \n",
    "            \n",
    "    def _build_n_grams(self, n: int, corpus: list[list[str]]) -> defaultdict:     \n",
    "        n_grams = defaultdict(int)       \n",
    "        \n",
    "        for text in corpus:\n",
    "            text.append(self._eos_token)\n",
    "            left_pointer = 0\n",
    "            right_pointer = n - 1\n",
    "            \n",
    "            while right_pointer < len(text):\n",
    "                middle_pointer = left_pointer\n",
    "                while middle_pointer != right_pointer:\n",
    "                    n_grams[tuple(text[left_pointer: middle_pointer + 1])] += 1\n",
    "                    middle_pointer += 1\n",
    "                    \n",
    "                n_grams[tuple(text[left_pointer: right_pointer + 1])] += 1\n",
    "                left_pointer += 1\n",
    "                right_pointer += 1\n",
    "            \n",
    "        return n_grams\n",
    "            \n",
    "            \n",
    "    def _next_word(self, text: list[str]):\n",
    "        if len(text) < self.n:\n",
    "            return f'To short sentence, has to be at leat of lenght: {self.n}'\n",
    "        \n",
    "        provided_context = text[-self.n:]\n",
    "        next_word = None\n",
    "        next_word_prob = -1\n",
    "        \n",
    "        for potential_next_word in self.vocab:\n",
    "            provided_context.append(potential_next_word)\n",
    "            phrase_prob = self.phrases.get(tuple(provided_context), 0)\n",
    "            provided_context.pop()\n",
    "            context_prob = self.context[tuple(provided_context)] if tuple(provided_context) in self.context else self._backoff(provided_context)\n",
    "            prob = phrase_prob / context_prob\n",
    "            \n",
    "            if prob > next_word_prob:\n",
    "                next_word_prob = prob\n",
    "                next_word = potential_next_word\n",
    "                \n",
    "        return next_word\n",
    "        \n",
    "    def fit(self, corpus: list[str]) -> None:\n",
    "        self._build_vocab(corpus)\n",
    "        self.context = self._build_n_grams(self.n, self._tokenized_text)\n",
    "        self.phrases = self._build_n_grams(self.n + 1, self._tokenized_text)\n",
    "        del self._tokenized_text\n",
    "        \n",
    "        \n",
    "    def generate(self, text: str, max_len: int = 20):\n",
    "        text = self.text_preprocessor.preprocess(text)\n",
    "        treshold = len(text) + max_len\n",
    "        next_word = None\n",
    "        \n",
    "        while next_word != self._eos_token and len(text) < treshold:\n",
    "            next_word = self._next_word(text)\n",
    "            if next_word != self._eos_token: text.append(next_word)\n",
    "            \n",
    "        return ' '.join(text)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"NGramsModel(n = {self.n}, text_preprocessor = {TextPreprocessor})\"\n",
    "        \n",
    "model = NGramsModel(n=3)\n",
    "model.fit(corpus)\n",
    "model.generate('and make intelligent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
