{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8406b6b2-5c38-42e1-bdd2-c0ebf5fc0307",
   "metadata": {},
   "source": [
    "## PyTorch exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466028e0-871c-4c72-868b-0b8439e659be",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "1. Make a tensor of size (2, 17)\n",
    "2. Make a torch.FloatTensor of size (3, 1)\n",
    "3. Make a torch.LongTensor of size (5, 2, 1)\n",
    "  - fill the entire tensor with 7s\n",
    "4. Make a torch.ByteTensor of size (5,)\n",
    "  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]\n",
    "5. Perform a matrix multiplication of two tensors of size (2, 4) and (4, 2). Then do it in-place.\n",
    "6. Do element-wise multiplication of two randomly filled $(n_1,n_2,n_3)$ tensors. Then store the result in an Numpy array.\n",
    "\n",
    "### Forward-prop/backward-prop\n",
    "1. Create a Tensor that `requires_grad` of size (5, 5).\n",
    "2. Sum the values in the Tensor.\n",
    "3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)\n",
    "4. Sum the variable's elements and assign to a new python variable\n",
    "5. Print the gradients of all the variables\n",
    "6. Now perform a backward pass on the last variable (NOTE: for each new python variable that you define, call `.retain_grad()`)\n",
    "7. Print all gradients again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d7d57-678c-4bf1-8754-06d1383e9884",
   "metadata": {},
   "source": [
    "### Deep-forward NNs\n",
    "1. Look at Lab 3. In Exercise 12 there, you had to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. Reimplement the manual code in PyTorch.\n",
    "2. Compare test accuracy using different optimizers: SGD, Adam, Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366761b-c026-4fba-a64e-07c57fef0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/6y59tr5d45s1nynfg_sv5zr80000gn/T/ipykernel_66240/2662333334.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_1 = torch.tensor(torch.randn(2, 17))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Tensor of size (2, 17)\n",
    "tensor_1 = torch.tensor(torch.randn(2, 17))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266054a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FloatTensor of size (3, 1)\n",
    "float_tensor = torch.FloatTensor(3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ff8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LongTensor of size (5, 2, 1) filled with 7s\n",
    "long_tensor = torch.LongTensor(5, 2, 1).fill_(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf1d8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ByteTensor of size (5,) with [0, 1, 1, 1, 0]\n",
    "byte_tensor = torch.ByteTensor([0, 1, 1, 1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74edf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Matrix multiplication (2, 4) x (4, 2), then in-place\n",
    "A = torch.randn(2, 4)\n",
    "B = torch.randn(4, 2)\n",
    "matmul_result = torch.mm(A, B)\n",
    "# In-place version (stored in a pre-allocated tensor)\n",
    "C = torch.empty(2, 2)\n",
    "torch.mm(A, B, out=C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fdc4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Element-wise multiplication of random (n1, n2, n3) tensors → store as NumPy\n",
    "n1, n2, n3 = 3, 4, 2\n",
    "t1 = torch.randn(n1, n2, n3)\n",
    "t2 = torch.randn(n1, n2, n3)\n",
    "elementwise_product_np = (t1 * t2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce9cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tensor with requires_grad\n",
    "x = torch.randn(5, 5, requires_grad=True)\n",
    "x.retain_grad()  # retain gradient for x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5193fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sum the values in x\n",
    "x_sum = x.sum()\n",
    "x_sum.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e9076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Multiply by 2 → assign to new variable\n",
    "x_times_2 = x * 2\n",
    "x_times_2.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783d4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sum elements of new variable\n",
    "final_sum = x_times_2.sum()\n",
    "final_sum.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b612f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward:\n",
      "x.grad: None\n",
      "x_sum.grad: None\n",
      "x_times_2.grad: None\n",
      "final_sum.grad: None\n"
     ]
    }
   ],
   "source": [
    "# 5. Print gradients before backward\n",
    "print(\"Before backward:\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"x_sum.grad:\", x_sum.grad)\n",
    "print(\"x_times_2.grad:\", x_times_2.grad)\n",
    "print(\"final_sum.grad:\", final_sum.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f048a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Perform backward pass\n",
    "final_sum.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da636b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After backward:\n",
      "x.grad:\n",
      " tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "x_sum.grad:\n",
      " None\n",
      "x_times_2.grad:\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "final_sum.grad:\n",
      " tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 7. Print gradients after backward\n",
    "print(\"\\nAfter backward:\")\n",
    "print(\"x.grad:\\n\", x.grad)\n",
    "print(\"x_sum.grad:\\n\", x_sum.grad)\n",
    "print(\"x_times_2.grad:\\n\", x_times_2.grad)\n",
    "print(\"final_sum.grad:\\n\", final_sum.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f80ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
